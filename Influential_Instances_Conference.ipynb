{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0IfoUOQ+7Syqy//ItIhl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csymvoul/influential-instances-identification/blob/iinstances/Influential_Instances_Conference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Deletion Diagnostics for the identification of Influential Instances towards Causal-based learning\n",
        "\n",
        "\n",
        "The steps needed to train an AI model based on the influential instances are:\n",
        "\n",
        "1. Train the model on the original dataset\n",
        "2. Calculate the DFBETA and RMSE values for each instance: \n",
        " * $DFBETA_{i}=\\beta-\\beta^{-i}$, where $\\beta$ is the weight vector of the initial model that is trained with all instances and $\\beta^{-i}$ is the weight vector of the initial model that is trained without the $i$-th instance.\n",
        " * The $\\beta^{-i}$ is calculated by training the model on the dataset without the $i$-th instance.\n",
        " * $RMSE_i = \\sqrt{\\frac{1}{n} \\times \\sum_{j=1}^{n}{(y_j - \\hat{y_j})^2}}$, where {i} is the removed instance, $n$ is the number of instances, $y_j$ is the correct value $\\hat{y_j}$ is the predicted value.\n",
        "\n",
        "3. Once the $DFBETA$ and $RMSE$ values are calculated, the influential instances are the ones with the highest $DFBETA$ and lowest $RMSE$ values\n",
        "4. Then only the influential instances are kept from the original dataset\n",
        "5. Consequently, we apply the K-Means algorithm, where $K$ is equal to the number influential instances and we apply it to the excluded data of the original dataset\n",
        "6. The centroids of the K-Means algorithm are the influential instances\n",
        "7. We cluster the original dataset based on the centroids of the K-Means algorithm and we produce a new final dataset, which we use to re-train the model \n",
        "\n"
      ],
      "metadata": {
        "id": "RwoNHZ77j1Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and connect to Google Drive"
      ],
      "metadata": {
        "id": "rzChTc1mjWsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries "
      ],
      "metadata": {
        "id": "swmIgVQAjqXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeFb648BjRVI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "hBieZ_qjs3Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "EHM_R811s5i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten, MaxPooling3D, Flatten\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, Conv3D"
      ],
      "metadata": {
        "id": "5bYXBkKLs2FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "XB2TufBms0ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install tslearn"
      ],
      "metadata": {
        "id": "WFSbQwScfTSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tslearn.metrics import dtw"
      ],
      "metadata": {
        "id": "yuseqhHefVcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Google Drive"
      ],
      "metadata": {
        "id": "sT1_a5MNjl-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HofM_4WQjutb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a54a31e-c425-4032-9338-4893db34c0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset"
      ],
      "metadata": {
        "id": "-8jZDNefo1f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://raw.githubusercontent.com/jenfly/opsd/master/opsd_germany_daily.csv'\n",
        "data = pd.read_csv(url,sep=\",\")\n",
        "print(data)"
      ],
      "metadata": {
        "id": "TfctN8BBo5O0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing"
      ],
      "metadata": {
        "id": "8DATolU5p5QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if there exist any rows with 0 values \n",
        "\n",
        "print(data.loc[data['Consumption'] == 0])\n",
        "print(data.loc[data['Wind'] == 0])\n",
        "print(data.loc[data['Solar'] == 0])\n",
        "print(data.loc[data['Wind+Solar'] == 0])"
      ],
      "metadata": {
        "id": "XVlRWw6Kpdgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Date as the index \n",
        "data = data.set_index('Date')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "QW-kZpV-p_eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of NaNs \n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "rMdEOsbJqG27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_without_nan = data.fillna(0)"
      ],
      "metadata": {
        "id": "sknJNlTwqPDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_without_nan\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9QtQMf-zqRnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model "
      ],
      "metadata": {
        "id": "NqywKMOqqWay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert series to supervised"
      ],
      "metadata": {
        "id": "PFPC9vfdqbuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    if isinstance(data, list):\n",
        "        n_vars = 1\n",
        "    else:\n",
        "        n_vars = data.shape[1]\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        pass\n",
        "    else:\n",
        "        data = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    print(n_vars)\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        print(i)\n",
        "        cols.append(data.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(data.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\n",
        "    # put it all together\n",
        "    agg = pd.concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    #cols_to_use = names[:len(names) - (n_out)]\n",
        "    #agg = agg[cols_to_use]\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "\n",
        "new_df = series_to_supervised(data,n_in= 1, n_out=1)"
      ],
      "metadata": {
        "id": "qmHcfosPqbD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_df.head())"
      ],
      "metadata": {
        "id": "T1vHtttEqn60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Min Max scaler"
      ],
      "metadata": {
        "id": "AacGSsOHqqbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Min_max_scal(data):\n",
        "\tarray = data.values\n",
        "\tvalues_ = array.astype('float32')\n",
        "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\tscaled = scaler.fit_transform(values_)\n",
        "\treturn scaled"
      ],
      "metadata": {
        "id": "hcGBr2wXqsD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = new_df.columns\n",
        "scaled_np = Min_max_scal(new_df)\n",
        "scaled_df = pd.DataFrame(scaled_np, columns=[columns])\n",
        "\n",
        "scaled_df.head()"
      ],
      "metadata": {
        "id": "fVO29kaQquHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split into train and test datasets"
      ],
      "metadata": {
        "id": "xSZUgxU0q6Rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_data_single_lag(reframed, train_percentage, test_percentage, valid_percentage):\n",
        "\t# split into train and test sets\n",
        "\tvalues = reframed.values\n",
        "\t# Sizes\n",
        "\ttrain_size = int(len(reframed) * train_percentage)\n",
        "\ttest_size = int(len(reframed) * test_percentage)\n",
        "\tvalid_size = int(len(reframed) * valid_percentage)\n",
        "\n",
        "\ttrain = values[:train_size]\n",
        "\ttest = values[train_size:train_size + test_size]\n",
        "\tval = values[train_size + test_size:]\n",
        "\n",
        "\t# split into input and outputs\n",
        "\ttrain_X, train_y = train[:, :-1], train[:, -1]\n",
        "\ttest_X, test_y = test[:, :-1], test[:, -1]\n",
        "\tval_X, val_y = val[:, :-1], val[:, -1]\n",
        "\t# print(train_X.shape)\n",
        "\n",
        "\t### this reshape below is we using it for univariate timeseries\n",
        "\t# reshape input to be 3D [samples, timesteps, features]\n",
        "\ttrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "\ttest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "\tval_X = val_X.reshape((val_X.shape[0], 1, val_X.shape[1]))\n",
        "\n",
        "\tprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape, val_X.shape, val_y.shape)\n",
        "\n",
        "\treturn train_X, train_y, test_X, test_y, val_X, val_y"
      ],
      "metadata": {
        "id": "5aTmA8Yxq-FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKBowSk3rBMp",
        "outputId": "58039482-a3a5-4662-9c27-dd2fa90bd3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2183, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_df1  = scaled_df[:1000]\n",
        "train_X, train_y, test_X, test_y, val_X, val_y = reshape_data_single_lag(scaled_df1,  0.65, 0.25, 0.10 )b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd9GpNinrJgz",
        "outputId": "b0449f1f-0dcd-4ef4-ebee-e9be3e77b74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(650, 1, 7) (650,) (250, 1, 7) (250,) (100, 1, 7) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/train_X.npy', train_X)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/train_y.npy', train_y)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/test_X.npy', test_X)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/test_y.npy', test_y)"
      ],
      "metadata": {
        "id": "tXXiexaSrNqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "kotVHY_hr0HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_model(train_X, train_y, test_X, test_y):\n",
        "    # design network\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(90, return_sequences = True,  input_shape=(train_X.shape[1], train_X.shape[2])))  # 1 , 2\n",
        "    model.add(Dropout(0.75))\n",
        "    model.add(LSTM(70, return_sequences = True,  input_shape=(train_X.shape[1], train_X.shape[2])))  # 1 , 2\n",
        "    model.add(Dropout(0.75))\n",
        "    model.add(LSTM(50, return_sequences = True,  input_shape=(train_X.shape[1], train_X.shape[2])))  # 1 , 2\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(LSTM(30, return_sequences = True,  input_shape=(train_X.shape[1], train_X.shape[2])))  # 1 , 2\n",
        "    model.add(Dropout(0.50))\n",
        "    model.add(LSTM(10, return_sequences = False,  input_shape=(train_X.shape[1], train_X.shape[2])))  # 1 , 2\n",
        "    model.add(Dropout(0.10))\n",
        "    #model.add(LSTM(15, return_sequences = False ))\n",
        "    #model.add(Dropout(0.5))\n",
        "    #model.add(LSTM(30, return_sequences = False ))\n",
        "    #model.add(Dropout(0.2))\n",
        "    #model.add(LSTM(15, return_sequences = False ))\n",
        "    #model.add(Dense(50))\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    # fit network\n",
        "    model.fit(train_X, train_y, epochs=25, batch_size=8, validation_data=(test_X, test_y),verbose=2, shuffle=False)\n",
        "    return model"
      ],
      "metadata": {
        "id": "eP9YwBSRr1s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM_model(train_X, train_y, test_X, test_y)"
      ],
      "metadata": {
        "id": "QOQxNIrcr5qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model summary "
      ],
      "metadata": {
        "id": "PtpCnIltiZoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "qKck1vRKr-jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot model loss"
      ],
      "metadata": {
        "id": "85lf4_uoib7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_test_loss(model):\n",
        "    mpl.rcParams['figure.figsize'] = (12, 8)\n",
        "    mpl.rcParams['axes.grid'] = False\n",
        "    \n",
        "    # plot history\n",
        "    plt.plot(model.history.history['loss'], label='train')\n",
        "    plt.plot(model.history.history['val_loss'], label='test')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Jz6HkZt6sCTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_test_loss(model)"
      ],
      "metadata": {
        "id": "OeTsdQnFsHol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions and scores "
      ],
      "metadata": {
        "id": "Xk1quvoEssD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictions_and_scores_for_LSTM(model, test_X,test_y):\n",
        "\t# make a prediction\n",
        "\tyhat = model.predict(test_X)\n",
        "\t# test_X_reshaped = test_X.reshape((test_X.shape[0], 3*2))\n",
        "\tyhat_reshaped = yhat.reshape((yhat.shape[0], yhat.shape[1]))\n",
        "\n",
        "\ttest_y_reshaped = test_y.reshape((len(test_y), 1))\n",
        "\n",
        "\t# calculate RMSE and R2_score\n",
        "\trmse = sqrt(mean_squared_error(test_y_reshaped, yhat_reshaped))\n",
        "\tr2score = r2_score(test_y_reshaped, yhat_reshaped)\n",
        "\t\n",
        "\tprint('Test RMSE: %.3f' % rmse)\n",
        "\tprint('R2_score: %f' % r2score)\n",
        "\treturn rmse, r2score"
      ],
      "metadata": {
        "id": "qsrHa6JosuXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_and_scores_for_LSTM(model, test_X, test_y)"
      ],
      "metadata": {
        "id": "mx54m-dhs9nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Influential Instances identification"
      ],
      "metadata": {
        "id": "JX-BSMHpzQFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The weights of the first model without deleting one or more instances\n",
        "initial_model_weights = model.get_weights()\n",
        "len(initial_model_weights)"
      ],
      "metadata": {
        "id": "awErCzekzUdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate $\\beta$, $RMSE$ and $R^2$"
      ],
      "metadata": {
        "id": "zoeOfMqAzdCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights = []\n",
        "rmse = []\n",
        "r2score = []\n",
        "\n",
        "j = 0\n",
        "for index, i in enumerate(train_X):\n",
        "  train_X_without_i = np.delete(train_X, index, 0)\n",
        "  train_y_without_i = np.delete(train_y, index, 0)\n",
        "  model_without_i = LSTM_model(train_X_without_i, train_y_without_i, test_X, test_y)\n",
        "  rmse_without_i, r2score_without_i = predictions_and_scores_for_LSTM(model_without_i, train_X_without_i, train_y_without_i)\n",
        "  model_weights += model_without_i.get_weights()\n",
        "  rmse.append(rmse_without_i)\n",
        "  r2score.append(r2score_without_i)\n",
        "  j+=1\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "HtTHd3Reznpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store scores into files"
      ],
      "metadata": {
        "id": "xj5OFMJ4z540"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/r2_score_list.txt\", \"w\") as f:\n",
        "    for s in r2score:\n",
        "        f.write(str(s) +\"\\n\")\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/r2_score_list.txtrmse_list.txt\", \"w\") as f:\n",
        "    for s in rmse:\n",
        "        f.write(str(s) +\"\\n\")"
      ],
      "metadata": {
        "id": "nPiyFwn8zxfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $DFBETA$ calculation"
      ],
      "metadata": {
        "id": "Wq2vJ2rw0XQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_weights = int(len(model_weights)/len(initial_model_weights))\n",
        "count=0\n",
        "# print(len(model_weights))\n",
        "# print(len(initial_model_weights))\n",
        "\n",
        "# for i in range(0, 2):\n",
        "count = 0\n",
        "dfbeta_all = []\n",
        "dfbeta_sum = []\n",
        "for i in range(0, len(model_weights)):\n",
        "  dfbeta = np.subtract(initial_model_weights[i-count], model_weights[i])\n",
        "  dfbeta_all.append(dfbeta)\n",
        "  if (i % len(initial_model_weights)==0):\n",
        "    count+=len(initial_model_weights)\n",
        "    dfbeta_sum.append(np.sum(dfbeta))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/dfbeta.csv\", \"w\") as f:\n",
        "    for s in dfbeta_sum:\n",
        "        f.write(str(s) +\"\\n\")\n",
        "\n",
        "\n",
        "dfbeta_sum_abs = np.abs(dfbeta_sum)\n",
        "dfbeta_sum = np.array(dfbeta_sum)"
      ],
      "metadata": {
        "id": "vhxfGI6t0b1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_influential_instances_indices = []\n",
        "number_of_influential_instances = len(dfbeta_sum)\n",
        "for i in range(0, number_of_influential_instances):\n",
        "  most_influential_instances_indices.append(np.argmax(dfbeta_sum))\n",
        "  dfbeta_sum[np.argmax(dfbeta_sum)] = -1000\n",
        "\n",
        "most_influential_instances_indices"
      ],
      "metadata": {
        "id": "GQHln1Cv0h9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "influential_instances = []\n",
        "for i in range(0, len(most_influential_instances_indices)):\n",
        "  influential_instances.append(scaled_df1.values.tolist())\n",
        "\n",
        "influential_instances"
      ],
      "metadata": {
        "id": "XBG-FKc_0lKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sort instances according to $RMSE$"
      ],
      "metadata": {
        "id": "ryLe260Y0pQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing RMSE to a file \n",
        "rmse_np = np.array(rmse)\n",
        "\n",
        "np.savetxt(\"/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/rmse.csv\", rmse_np, delimiter=\",\")"
      ],
      "metadata": {
        "id": "bB8-7nj800Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_np = np.array(rmse)\n",
        "least_rmse_indices = []\n",
        "for i in range(0, len(rmse)):\n",
        "  least_rmse_indices.append(np.argmin(rmse_np))\n",
        "  rmse_np[np.argmin(rmse_np)] = 99999\n",
        "\n",
        "least_rmse_indices"
      ],
      "metadata": {
        "id": "u9PQkQLp1HWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sort instances according to $R^2$"
      ],
      "metadata": {
        "id": "5jcOzGlz1IuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing R2 to a file \n",
        "r2_score_np = np.array(r2_score)\n",
        "\n",
        "numpy.savetxt(\"/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/r2_score.csv\", r2_score_np, delimiter=\",\")"
      ],
      "metadata": {
        "id": "mru3Os3G1Mke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2score_np = np.array(r2score)\n",
        "highest_r2score_indices = []\n",
        "for i in range(0, len(r2score)):\n",
        "  highest_r2score_indices.append(np.argmax(r2score_np))\n",
        "  r2score_np[np.argmax(r2score_np)] = -100\n",
        "\n",
        "highest_r2score_indices"
      ],
      "metadata": {
        "id": "heJLkbh51ReA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge $RMSE$, $R^2$, and $DFBETA$ to one dataframe"
      ],
      "metadata": {
        "id": "bhhRz3ul1UoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = np.column_stack([most_influential_instances_indices, least_rmse_indices, highest_r2score_indices])\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "from google.colab import files\n",
        "metrics_df.to_csv('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/metrics.csv')\n",
        "files.download('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/metrics.csv')"
      ],
      "metadata": {
        "id": "TtXilJvH1cjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Store metrics to csv files"
      ],
      "metadata": {
        "id": "Xsn1wfVq2QnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r2score_df =  pd.DataFrame(r2score)\n",
        "\n",
        "from google.colab import files\n",
        "r2score_df.to_csv('r2score.csv')\n",
        "files.download('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/r2score.csv')"
      ],
      "metadata": {
        "id": "303J0i8o2Tqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_df =  pd.DataFrame(rmse)\n",
        "\n",
        "from google.colab import files\n",
        "rmse_df.to_csv('rmse.csv')\n",
        "files.download('rmse.csv')"
      ],
      "metadata": {
        "id": "c_2A98qL2dEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights_df = pd.DataFrame(model_weights)\n",
        "\n",
        "from google.colab import files\n",
        "model_weights_df.to_csv('model_weights.csv')\n",
        "files.download('model_weights.csv')"
      ],
      "metadata": {
        "id": "YTgz7uQo2feM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a DataFrame with $DFBETA$, and $RMSE$\n",
        "\n"
      ],
      "metadata": {
        "id": "9p8q2g7-dkYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfbeta = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/dfbeta.csv', delimiter=',')\n",
        "rmse = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/rmse_list.txt', delimiter=',')\n",
        "r2_score = np.genfromtxt('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/results/r2_score_list.txt', delimiter=',')"
      ],
      "metadata": {
        "id": "rhkdozdNeaZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame()\n",
        "metrics_df['rmse'] = rmse.tolist()\n",
        "metrics_df['dfbeta'] = dfbeta.tolist()\n",
        "metrics_df['dfbeta_abs'] = abs(metrics_df['dfbeta'])\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "bJ9DGLiseVVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfbeta_accepted_value = dfbeta.std()+dfbeta.mean()\n",
        "rmse_accepted_value = rmse.std()+rmse.mean()\n",
        "\n",
        "filtered_metrics_df = metrics_df[\n",
        "                           (metrics_df['dfbeta_abs'] > dfbeta_accepted_value) & (metrics_df['rmse'] > rmse_accepted_value)\n",
        "                        ]\n",
        "print(f\"Number of identified Influential Instances: {len(filtered_metrics_df)}\")\n",
        "filtered_metrics_df.head()"
      ],
      "metadata": {
        "id": "6tcIk8hSfE3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting indices of Influential Instances"
      ],
      "metadata": {
        "id": "2pGMxovwfJRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices_of_influential_instances = filtered_metrics_df.index\n",
        "indices_of_influential_instances.tolist()"
      ],
      "metadata": {
        "id": "xAZbBEl8fNdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "influential_instances = np.array(train_X)[indices_of_influential_instances]"
      ],
      "metadata": {
        "id": "OYl6YNQSfQIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating distances using the Dynamic Time Warping Matching metric"
      ],
      "metadata": {
        "id": "FMisTA0efg_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtw_distance = []\n",
        "\n",
        "for index, instance in enumerate(train_X):\n",
        "    for index1, instance1 in enumerate(train_X):\n",
        "        dtw_distance.append(dtw(instance, instance1))"
      ],
      "metadata": {
        "id": "FWvAak88fpfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_of_influential_instances = indices_of_influential_instances.tolist()\n",
        "indices_of_influential_instances"
      ],
      "metadata": {
        "id": "YYqrp-SVftnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold_distance = 0.3*np.mean(dtw_distance)+0.1*np.std(dtw_distance)\n",
        "print(f'{threshold_distance}')\n",
        "\n",
        "newly_identified_influential_instances = []\n",
        "for ii_index, influential_instance in enumerate(influential_instances):\n",
        "    for i_index, instance in enumerate(train_X):\n",
        "        distance = dtw(influential_instance, instance)\n",
        "\n",
        "        if (distance <= threshold_distance and distance != 0 and i_index not in newly_identified_influential_instances):\n",
        "            newly_identified_influential_instances.append(i_index)\n",
        "            indices_of_influential_instances.append(i_index)"
      ],
      "metadata": {
        "id": "aMPfpHt8fvrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_of_influential_instances_without_duplicates = list(dict.fromkeys(indices_of_influential_instances))\n",
        "len(indices_of_influential_instances_without_duplicates)"
      ],
      "metadata": {
        "id": "O3B_FhN1f24G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataset that only includes influential instances"
      ],
      "metadata": {
        "id": "AlbwUdBvfrx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "influential_instances_X = np.array(train_X)[indices_of_influential_instances_without_duplicates]\n",
        "influential_instances_y = np.array(train_y)[indices_of_influential_instances_without_duplicates]"
      ],
      "metadata": {
        "id": "E7roDc6Wf5Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Storing new dataset"
      ],
      "metadata": {
        "id": "Cw2JLl9SgANR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/influential_instances_train_X.npy', influential_instances_X)\n",
        "np.save('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/influential_instances_train_y.npy', influential_instances_y)"
      ],
      "metadata": {
        "id": "Z5571g2Pf-zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-train LSTM model with new influential instances dataset"
      ],
      "metadata": {
        "id": "QkPQHz3QgOv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Retrieving train and test sets"
      ],
      "metadata": {
        "id": "k5ISWRoRhPC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = np.load('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/influential_instances_train_X.npy')\n",
        "train_y = np.load('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/influential_instances_train_y.npy')\n",
        "test_X = np.load('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/test_X.npy')\n",
        "test_y = np.load('/content/drive/MyDrive/Colab Notebooks/influential_instances_conf/datasets/test_y.npy')"
      ],
      "metadata": {
        "id": "eE-HB167hOfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM_model(train_X, train_y, test_X, test_y)"
      ],
      "metadata": {
        "id": "pP4HIQubgOGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model summary"
      ],
      "metadata": {
        "id": "g6C4U0qciJph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "nXa2LjLqh48U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot model loss "
      ],
      "metadata": {
        "id": "P1MpolNriAIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_test_loss(model)"
      ],
      "metadata": {
        "id": "HtNQNUXKh7Tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}